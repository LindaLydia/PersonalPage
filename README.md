# PersonalPage

A repository with only readme.md that presents my experience



## Research Experience

1. **LightWrite**: teach Handwriting to The Visually Impaired with A Smartphone

   * I participated in the project LightWrite and created an Android app to teach the visually impaired group to write lowercase English letters and Arabian digits. 

   * My main work in this study is to collect training data from volunteers and use NN to construct a classifier for identifying and distinguishing the characters written by app users. It performs well in the study, with an accuracy over 90%.

   * ![the data for model training collected from volunteers, letter a is written in the we designed for visually impaired people, aberrant from the usual handwritten form of a](C:\Users\LYDIA\AppData\Roaming\Typora\typora-user-images\image-20210206222110894.png)

     â€‹	(the data for model training collected from volunteers, letter a is written in the we designed for visually impaired people, aberrant from the usual handwritten form of a)

   * I also helped with the user study and designed an app tutorial which is implemented in the app and has received positive feedbacks from visually impaired participants.

   * Relevant paper has been submitted to CHI 2021 and is currently under review and rebuttal. 

   * Further study is going on and hoping to introduce this app to the market.

   * **Sorry, the video is not available yet, since the paper is still under review and rebuttal.**

   

2. Home Automation device selection with **Sight Glance** based on VR

   * This a team project for the curriculum **Human Computer Interaction Theory and Technology**.
   * We first created a VR scene and used Oculus2 to collect the original vision point of 14 users of 10 objects in the scene. These points are then used to create a Bayes Model to help predict what the user is looking at even with his/her eyes off the object.
   * ![](./picture/SightGlance-scene.png)
   * We compared the system implemented with our Bayes model with the system using merely "vision collision" with 10 volunteers using both systems and saw a considerable improvement in both the user experience(more natural and easy to meet intention) and interactive efficiency. 
   * ![](./picture/SightGlance-result.png)
   * I designed of the 2 user study in this work and did all the processed all the data in the 2 studies, including fitting the collected vision points with 2D Gaussian Fitting as well as processing and graphing the data from the second study.

   

3. 